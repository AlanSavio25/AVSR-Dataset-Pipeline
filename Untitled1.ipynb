{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1afb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(\"output_data/*/pyavi/tracks/*.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19834873",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(range(1, 10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aff4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        super(VideoIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.mp4s = []\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                utt, _, _, _, _, mp4, _ = line.strip().split(None, 6)\n",
    "                print(utt)\n",
    "                print(mp4)\n",
    "                self.utts.append(utt)\n",
    "                self.mp4s.append(mp4)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        print(f\"worker info: {worker_info}\")\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.load_frames(self.mp4s[i])\n",
    "    \n",
    "    def load_frames(self, videofile):\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "        frame_num = 1;\n",
    "        frames = []\n",
    "        while frame_num:\n",
    "            frame_num += 1\n",
    "            ret, image = cap.read()\n",
    "            if ret == 0:\n",
    "                break\n",
    "            frames.append(cv2.resize(image, (224, 224)))\n",
    "        frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "        start = time.time()\n",
    "        frames = np.stack(frames, axis=3)\n",
    "        frames = np.transpose(frames, (2,3,0,1))\n",
    "        frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "        end = time.time()\n",
    "        timetaken = end-start\n",
    "        print(f\"Time taken to reshape: {timetaken}\")\n",
    "        print(f\"frame shape: {frames[0].shape}\")\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73613bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoIterableDataset('%s/wav.scp' % directory)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3709384",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob('/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open( '/group/cstr/datawww/asru/MGB1/data/scoring/task2_eval.ref.ctm', 'r')\n",
    "lines = f.read()\n",
    "print(lines)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/eval.task1~ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"20080505_180000_bbcfour_the_book_quiz\"\n",
    "evalfiles = glob.glob(f'/afs/inf.ed.ac.uk/group/project/nst/bbcdata/**/*{filename}*.ts', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(\" /afs/inf.ed.ac.uk/group/project/summa/MGB1/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16ce07",
   "metadata": {},
   "source": [
    "# Dataloader stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c718eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import cv2\n",
    "import time, glob, shutil, datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import csv, json\n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from syncnet_python.facetrack import *\n",
    "from syncnet_python.syncnet import *\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4baa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        super(VideoIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.avis = []\n",
    "        for utt in glob.glob(data_dir+'*'):\n",
    "            self.utts.append(utt)\n",
    "            avi = utt+'/pyavi/tracks/video.avi'\n",
    "            self.avis.append(avi)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.load_frames(self.avis[i])\n",
    "    \n",
    "    def load_frames(self, videofile):\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "        frame_num = 1;\n",
    "        frames = []\n",
    "        while frame_num:\n",
    "            frame_num += 1\n",
    "            ret, image = cap.read()\n",
    "            if ret == 0:\n",
    "                break            \n",
    "            frames.append(image)\n",
    "        return frames\n",
    "    \n",
    "def prepare_output_directory(location):\n",
    "    ready = True\n",
    "    does_not_require_processing = False\n",
    "    incomplete_directory_exists = os.path.isdir(location) and not os.path.exists(f\"{location}/utterance_info.csv\")\n",
    "    if(incomplete_directory_exists):\n",
    "        shutil.rmtree(location)\n",
    "    elif(os.path.isdir(location)):\n",
    "        return does_not_require_processing  #  This utterance has been processed already. Continuing to next utterance..\n",
    "    else:\n",
    "        pass\n",
    "    subprocess.run(\"mkdir -p \" + location + \"/pyavi/tracks/\", stdout=subprocess.DEVNULL, shell=True)    \n",
    "    return ready\n",
    "\n",
    "def create_transcript_from_XML(location, item):\n",
    "    # TODO: the transcript should only contain the words spoken in the final cropped video. \n",
    "    utterance = \"\"\n",
    "    for child in item:\n",
    "        utterance+=child.text + \" \"\n",
    "    data = item.attrib\n",
    "    data.update({\"utterance\": utterance})\n",
    "    with open(location + '/transcript.txt', 'w') as outfile:\n",
    "        outfile.write(str(data))\n",
    "\n",
    "def cut_into_utterances(filename, output_dir, maxWMER=1000):\n",
    "    \n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml/\"\n",
    "    xmlfile = xmldir + filename + \".xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    utterance_items = []\n",
    "    paths = glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/ptn*/**/{filename}*.ts\") + glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/raw/{filename}*.ts\")\n",
    "    inputVideo = paths[0]\n",
    "    command_elems = [\"ffmpeg -loglevel quiet -y -i \" + inputVideo]\n",
    "    for item in root.findall('./body/segments/segment'):\n",
    "        if (item.attrib['id'].split('_')[-1]=='align' and float(item.attrib['WMER'])<=maxWMER):\n",
    "            if (float(item.attrib['endtime']) - float(item.attrib['starttime'])<2):\n",
    "                continue                        \n",
    "            location = output_dir + item.attrib['id']\n",
    "            reference = \"tracks\"\n",
    "            status = prepare_output_directory(location)\n",
    "            if status:\n",
    "                utterance_items.append(item)\n",
    "                data = item.attrib\n",
    "                start = datetime.timedelta(seconds=float(data['starttime']))\n",
    "                end = datetime.timedelta(seconds=float(data['endtime']))\n",
    "                output = location + '/pyavi/tracks/video.avi'\n",
    "                command_elems.append(\" -ss \" + str(start) + \" -to \" + str(end) + \" -c copy \" + output)\n",
    "                create_transcript_from_XML(location, item)\n",
    "    command = \"\".join(command_elems)\n",
    "    s = time.time()\n",
    "    result = subprocess.run(command, shell=True, stdout=None)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"ERROR: ffmpeg failed to trim video: {filename}\")\n",
    "        print(f\"result: {result}\")\n",
    "    t = time.time() - s\n",
    "    print(f\"Took {t} seconds to trim {len(command_elems)-1} utterances\")\n",
    "    return utterance_items\n",
    "\n",
    "def getGenre(filename):\n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml/\"\n",
    "    xmlfile = xmldir + filename + \".xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    head = root.find('./head/recording')\n",
    "    genre = head.attrib[\"genre\"]\n",
    "    return genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcbb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/disk/scratch/s1768177/pipeline/output_data/'\n",
    "filelist = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/train.short\"\n",
    "desired_genres = [\"drama\", \"childrens\", \"news\", \"documentary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a95904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-07-27 14:22:49.429436. Cutting utterances from raw videos.\n",
      "1. 20080505_000500_bbcone_weatherview. (news) \n",
      "Took 0.6894359588623047 seconds to trim 17 utterances\n",
      "\n",
      "Finished Cutting total 17 utterances from 1 videos\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "with open(filelist, \"r\") as f:\n",
    "    files = f.read().split()\n",
    "files = files[:1]\n",
    "print(f\"\\n{datetime.datetime.now()}. Cutting utterances from raw videos.\")\n",
    "total_utterances_processed = 0\n",
    "for filename in files:\n",
    "    start = time.time()\n",
    "    genre = getGenre(filename)\n",
    "    if (genre in desired_genres):\n",
    "        print(f\"{count}. {filename}. ({genre}) \")\n",
    "        count += 1\n",
    "        utterance_items = cut_into_utterances(filename, data_dir)\n",
    "        total_utterances_processed += len(utterance_items)\n",
    "print(f\"\\nFinished Cutting total {total_utterances_processed} utterances from {count-1} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae0e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S3FD] loading with cuda:1\n",
      "[S3FD] finished loading (3.9826 sec)\n",
      "0 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_9_align 224\n",
      "1 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align 643\n",
      "2 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_12_align 90\n",
      "3 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_4_align 281\n",
      "4 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_6_align 117\n",
      "5 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align 522\n",
      "6 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_13_align 251\n",
      "7 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_20_align 83\n",
      "8 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_2_align 124\n",
      "9 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_7_align 188\n",
      "10 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_19_align 129\n",
      "11 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_11_align 82\n",
      "12 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_21_align 51\n",
      "13 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_10_align 345\n",
      "14 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_3_align 267\n",
      "15 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_14_align 250\n",
      "16 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_5_align 190\n"
     ]
    }
   ],
   "source": [
    "dataset = VideoIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=16)\n",
    "facetrack = FaceTrack('cuda:1')\n",
    "for i, (utt, frames) in enumerate(dataloader):\n",
    "    print(i, utt, len(frames))\n",
    "    facetrack.run(data_dir=utt, frames=frames)\n",
    "    no_faces_found = len(os.listdir(utt + \"/pycrop/tracks/\")) == 0\n",
    "    if(no_faces_found):\n",
    "        shutil.rmtree(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1637dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyncNetIterableDataset(torch.utils.data.IterableDataset):    \n",
    "    def __init__(self, path):\n",
    "        super(SyncNetIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.avis = []\n",
    "        for avi in glob.glob(data_dir+'*/pycrop/tracks/*.avi'):\n",
    "            self.avis.append(avi)\n",
    "            utt = avi.split('/pycrop/')[0]\n",
    "            self.utts.append(utt)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.avis[i], self.load_frames(self.avis[i])\n",
    "    \n",
    "    def load_frames(self, videofile):\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "        frame_num = 1;\n",
    "        frames = []\n",
    "        while frame_num:\n",
    "            frame_num += 1\n",
    "            ret, image = cap.read()\n",
    "            if ret == 0:\n",
    "                break\n",
    "            frames.append(cv2.resize(image, (224, 224)))\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "        frames = np.stack(frames, axis=3)\n",
    "        frames = np.transpose(frames, (2,3,0,1))\n",
    "        frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5625a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model syncnet_python/data/syncnet_v2.model loaded.\n",
      "0 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pycrop/tracks/00001.avi 104\n",
      "1 9.404757\n",
      "1 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pycrop/tracks/00000.avi 422\n",
      "1 5.99399\n",
      "2 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_4_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_4_align/pycrop/tracks/00000.avi 175\n",
      "1 7.0156937\n",
      "3 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align/pycrop/tracks/00001.avi 117\n",
      "1 7.0394135\n",
      "4 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align/pycrop/tracks/00000.avi 405\n",
      "1 6.76659\n",
      "5 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_13_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_13_align/pycrop/tracks/00000.avi 174\n",
      "0 8.437675\n",
      "6 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_3_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_3_align/pycrop/tracks/00000.avi 141\n",
      "1 7.5887165\n",
      "7 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_14_align /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_14_align/pycrop/tracks/00000.avi 177\n",
      "1 6.7403455\n"
     ]
    }
   ],
   "source": [
    "dataset = SyncNetIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=16)\n",
    "syncnet = SyncNet()\n",
    "for i, (utt, avi, frames) in enumerate(dataloader):\n",
    "    print(i, utt, avi, len(frames))\n",
    "    syncnet.setup(utt)\n",
    "    offset, conf, dist = syncnet.evaluate(avi,frames)\n",
    "    print(offset, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e37921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3887e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd583dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59063e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

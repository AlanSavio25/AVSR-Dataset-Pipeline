{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1afb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(\"output_data/*/pyavi/tracks/*.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aff4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VideoIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "#     def __init__(self, path):\n",
    "#         super(VideoIterableDataset).__init__()\n",
    "#         self.utts = []\n",
    "#         self.mp4s = []\n",
    "#         with open(path, 'r') as f:\n",
    "#             for i, line in enumerate(f):\n",
    "#                 utt, _, _, _, _, mp4, _ = line.strip().split(None, 6)\n",
    "#                 print(utt)\n",
    "#                 print(mp4)\n",
    "#                 self.utts.append(utt)\n",
    "#                 self.mp4s.append(mp4)\n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         worker_info = torch.utils.data.get_worker_info()\n",
    "#         print(f\"worker info: {worker_info}\")\n",
    "#         if worker_info is None:\n",
    "#             offset = 0\n",
    "#             shift = 1\n",
    "#         else:\n",
    "#             offset = worker_info.id\n",
    "#             shift = worker_info.num_workers\n",
    "#         for i in range(offset, len(self.utts), shift):\n",
    "#             yield self.utts[i], self.load_frames(self.mp4s[i])\n",
    "    \n",
    "#     def load_frames(self, videofile):\n",
    "#         cap = cv2.VideoCapture(videofile)\n",
    "#         frame_num = 1;\n",
    "#         frames = []\n",
    "#         while frame_num:\n",
    "#             frame_num += 1\n",
    "#             ret, image = cap.read()\n",
    "#             if ret == 0:\n",
    "#                 break\n",
    "#             frames.append(cv2.resize(image, (224, 224)))\n",
    "#         frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "#         start = time.time()\n",
    "#         frames = np.stack(frames, axis=3)\n",
    "#         frames = np.transpose(frames, (2,3,0,1))\n",
    "#         frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "#         end = time.time()\n",
    "#         timetaken = end-start\n",
    "#         print(f\"Time taken to reshape: {timetaken}\")\n",
    "#         print(f\"frame shape: {frames[0].shape}\")\n",
    "#         return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73613bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = VideoIterableDataset('%s/wav.scp' % directory)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3709384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob('/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open( '/group/cstr/datawww/asru/MGB1/data/scoring/task2_eval.ref.ctm', 'r')\n",
    "# lines = f.read()\n",
    "# print(lines)\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/eval.task1~ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"20080505_180000_bbcfour_the_book_quiz\"\n",
    "# evalfiles = glob.glob(f'/afs/inf.ed.ac.uk/group/project/nst/bbcdata/**/*{filename}*.ts', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(\" /afs/inf.ed.ac.uk/group/project/summa/MGB1/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16ce07",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c718eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import cv2\n",
    "import time, glob, shutil, datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import csv, json\n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from syncnet_python.facetrack import *\n",
    "from syncnet_python.syncnet import *\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4baa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        super(VideoIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.avis = []\n",
    "        for utt in glob.glob(data_dir+'*'):\n",
    "            self.utts.append(utt)\n",
    "            avi = utt+'/pyavi/tracks/video.avi'\n",
    "            self.avis.append(avi)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.load_frames(self.utts[i])\n",
    "    \n",
    "    def load_frames(self, utt):\n",
    "        frames_dir = utt+'/pyframes/tracks/'\n",
    "        if os.path.exists(frames_dir):\n",
    "            rmtree(frames_dir)\n",
    "        os.makedirs(frames_dir)\n",
    "\n",
    "        input_video = os.path.join(utt,'pyavi/tracks','video.avi')\n",
    "        output = os.path.join(utt,'pyframes/tracks','%06d.jpg')\n",
    "        command = f\"ffmpeg -loglevel quiet -y -i {input_video} -qscale:v 2 -threads 1 -f image2 {output}\"\n",
    "        output = subprocess.call(command, shell=True, stdout=None)\n",
    "        flist = glob.glob(utt+'/pyframes/tracks/*.jpg')\n",
    "        flist.sort()\n",
    "        frames = []\n",
    "        for fname in flist:\n",
    "            image = cv2.imread(fname)\n",
    "            frames.append(image)\n",
    "#         cap = cv2.VideoCapture(videofile)\n",
    "#         frame_num = 1;\n",
    "#         frames = []\n",
    "#         while frame_num:\n",
    "#             frame_num += 1\n",
    "#             ret, image = cap.read()\n",
    "#             if ret == 0:\n",
    "#                 break            \n",
    "#             frames.append(image)\n",
    "#         print(f\"The shape of frames is: {np.array(frames).shape}\")\n",
    "        return frames\n",
    "    \n",
    "def prepare_output_directory(location):\n",
    "    ready = True\n",
    "    does_not_require_processing = False\n",
    "    incomplete_directory_exists = os.path.isdir(location) and not os.path.exists(f\"{location}/utterance_info.csv\")\n",
    "    if(incomplete_directory_exists):\n",
    "        shutil.rmtree(location)\n",
    "    elif(os.path.isdir(location)):\n",
    "        return does_not_require_processing  #  This utterance has been processed already. Continuing to next utterance..\n",
    "    else:\n",
    "        pass\n",
    "    subprocess.run(\"mkdir -p \" + location + \"/pyavi/tracks/\", stdout=subprocess.DEVNULL, shell=True)    \n",
    "    return ready\n",
    "\n",
    "def create_transcript_from_XML(location, item):\n",
    "    # TODO: the transcript should only contain the words spoken in the final cropped video. \n",
    "    utterance = \"\"\n",
    "    for child in item:\n",
    "        utterance+=child.text + \" \"\n",
    "    data = item.attrib\n",
    "    data.update({\"utterance\": utterance})\n",
    "    with open(location + '/transcript.txt', 'w') as outfile:\n",
    "        outfile.write(str(data))\n",
    "\n",
    "def cut_into_utterances(filename, output_dir, maxWMER=1000):\n",
    "    \n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml/\"\n",
    "    xmlfile = xmldir + filename + \".xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    utterance_items = []\n",
    "    paths = glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/ptn*/**/{filename}*.ts\") + glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/raw/{filename}*.ts\")\n",
    "    inputVideo = paths[0]\n",
    "    command_elems = [\"ffmpeg -loglevel quiet -y -i \" + inputVideo]\n",
    "    for item in root.findall('./body/segments/segment'):\n",
    "        if (item.attrib['id'].split('_')[-1]=='align' and float(item.attrib['WMER'])<=maxWMER):\n",
    "            if (float(item.attrib['endtime']) - float(item.attrib['starttime'])<2):\n",
    "                continue                        \n",
    "            location = output_dir + item.attrib['id']\n",
    "            reference = \"tracks\"\n",
    "            status = prepare_output_directory(location)\n",
    "            if status:\n",
    "                utterance_items.append(item)\n",
    "                data = item.attrib\n",
    "                start = datetime.timedelta(seconds=float(data['starttime']))\n",
    "                end = datetime.timedelta(seconds=float(data['endtime']))\n",
    "                output = location + '/pyavi/tracks/video.avi'\n",
    "                command_elems.append(\" -ss \" + str(start) + \" -to \" + str(end) + \" -c copy \" + output)\n",
    "                create_transcript_from_XML(location, item)\n",
    "    command = \"\".join(command_elems)\n",
    "    s = time.time()\n",
    "    result = subprocess.run(command, shell=True, stdout=None)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"ERROR: ffmpeg failed to trim video: {filename}\")\n",
    "        print(f\"result: {result}\")\n",
    "    t = time.time() - s\n",
    "    print(f\"Took {t} seconds to trim {len(command_elems)-1} utterances\")\n",
    "    return utterance_items\n",
    "\n",
    "def getGenre(filename):\n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml/\"\n",
    "    xmlfile = xmldir + filename + \".xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    head = root.find('./head/recording')\n",
    "    genre = head.attrib[\"genre\"]\n",
    "    return genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94b8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/disk/scratch/s1768177/pipeline/output_data/'\n",
    "filelist = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/train.short\"\n",
    "desired_genres = [\"drama\", \"childrens\", \"news\", \"documentary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9751dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-07-27 23:17:29.768246. Cutting utterances from raw videos.\n",
      "1. 20080505_000500_bbcone_weatherview. (news) \n",
      "Took 0.5648167133331299 seconds to trim 17 utterances\n",
      "\n",
      "Finished Cutting total 17 utterances from 1 videos\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "with open(filelist, \"r\") as f:\n",
    "    files = f.read().split()\n",
    "files = files[:1]\n",
    "print(f\"\\n{datetime.datetime.now()}. Cutting utterances from raw videos.\")\n",
    "total_utterances_processed = 0\n",
    "for filename in files:\n",
    "    start = time.time()\n",
    "    genre = getGenre(filename)\n",
    "    if (genre in desired_genres):\n",
    "        print(f\"{count}. {filename}. ({genre}) \")\n",
    "        count += 1\n",
    "        utterance_items = cut_into_utterances(filename, data_dir)\n",
    "        total_utterances_processed += len(utterance_items)\n",
    "print(f\"\\nFinished Cutting total {total_utterances_processed} utterances from {count-1} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae0e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S3FD] loading with cuda:0\n",
      "[S3FD] finished loading (0.3267 sec)\n",
      "0 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_9_align 233\n",
      "1 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align 655\n",
      "The shape of flist is: 655\n",
      "The shape of flist is: 655\n",
      "2 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_12_align 98\n",
      "3 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_4_align 286\n",
      "The shape of flist is: 286\n",
      "4 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_6_align 122\n",
      "5 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align 526\n",
      "The shape of flist is: 526\n",
      "The shape of flist is: 526\n",
      "The shape of flist is: 526\n",
      "6 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_13_align 261\n",
      "The shape of flist is: 261\n",
      "7 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_20_align 93\n",
      "8 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_2_align 137\n",
      "9 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_7_align 197\n",
      "10 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_19_align 142\n",
      "11 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_11_align 90\n",
      "12 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_21_align 65\n",
      "13 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_10_align 354\n",
      "14 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_3_align 279\n",
      "15 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_14_align 259\n",
      "16 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_5_align 203\n",
      "Time taken: 3.73 minutes\n"
     ]
    }
   ],
   "source": [
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "start = time.time()\n",
    "dataset = VideoIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=8)\n",
    "facetrack = FaceTrack('cuda:0')\n",
    "for i, (utt, frames) in enumerate(dataloader):\n",
    "    print(i, utt, len(frames))\n",
    "    facetrack.run(data_dir=utt, frames=frames)\n",
    "    no_faces_found = len(os.listdir(utt + \"/pycrop/tracks/\")) == 0\n",
    "    if(no_faces_found):\n",
    "        shutil.rmtree(utt)\n",
    "print(f\"Time taken: {(time.time()-start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0e8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyncNetIterableDataset(torch.utils.data.IterableDataset):    \n",
    "    def __init__(self, path):\n",
    "        super(SyncNetIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.avis = []\n",
    "        for avi in glob.glob(data_dir+'*/pycrop/tracks/*.avi'):\n",
    "            self.avis.append(avi)\n",
    "            utt = avi.split('/pycrop/')[0]\n",
    "            self.utts.append(utt)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.avis[i], self.load_frames(self.avis[i])\n",
    "    \n",
    "    def load_frames(self, videofile):\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "        frame_num = 1;\n",
    "        frames = []\n",
    "        while frame_num:\n",
    "            frame_num += 1\n",
    "            ret, image = cap.read()\n",
    "            if ret == 0:\n",
    "                break\n",
    "            frames.append(cv2.resize(image, (224, 224)))\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "        frames = np.stack(frames, axis=3)\n",
    "        frames = np.transpose(frames, (2,3,0,1))\n",
    "        frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5625a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model syncnet_python/data/syncnet_v2.model loaded.\n",
      "0 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align 104\n",
      "1 9.404757\n",
      "1 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align 157\n",
      "1 7.7370033\n",
      "2 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_4_align 175\n",
      "1 7.0156937\n",
      "3 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align 108\n",
      "1 7.929092\n",
      "4 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align 222\n",
      "0 7.223252\n",
      "5 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_15_align 117\n",
      "1 7.0394135\n",
      "6 /disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_13_align 174\n",
      "0 8.477043\n"
     ]
    }
   ],
   "source": [
    "dataset = SyncNetIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=16)\n",
    "syncnet = SyncNet()\n",
    "for i, (utt, avi, frames) in enumerate(dataloader):\n",
    "    print(i, utt, avi, len(frames))\n",
    "    syncnet.setup(utt)\n",
    "    offset, conf, dist = syncnet.evaluate(avi,frames)\n",
    "    print(offset, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd0953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3887e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videofile = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/tracks/video.mp4\"\n",
    "videofile_avi = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/tracks/video.avi\"\n",
    "\n",
    "cap = cv2.VideoCapture(videofile_avi)\n",
    "frames = []\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(frame_count)\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    if ret == 0:\n",
    "        break\n",
    "    frames.append(image)\n",
    "print(f\"The shape of frames is: {np.array(frames).shape}\")\n",
    "# for i in range(len(frames)):\n",
    "#     cv2.imwrite(f\"playground/cv2images/{i}.jpg\", frames[i])\n",
    "print(len(frames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd583dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# command = (\"ffmpeg -loglevel quiet -y -i %s -qscale:v 2 -async 1 -r 25 %s\" % (videofile, videofile_avi))\n",
    "# output = subprocess.call(command, shell=True, stdout=None)\n",
    "command = (\"ffmpeg -y -i %s -threads 1 -f image2 %s\" % (videofile_avi,'playground/images/%06d.jpg')) \n",
    "output = subprocess.call(command, shell=True)\n",
    "print(output)\n",
    "flist = glob.glob('playground/images/*.jpg')\n",
    "flist.sort()\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59063e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flist = glob.glob('playground/images/*.jpg')\n",
    "# flist.sort()\n",
    "# for image in flist:\n",
    "#     print(cv2.imread(image).shape)\n",
    "#     print(frames[50].shape)\n",
    "#     if (cv2.imread(image) == frames[50]).all():\n",
    "#         print(\"got it!\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052999be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455b695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

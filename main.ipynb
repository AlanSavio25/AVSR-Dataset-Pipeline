{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c718eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import cv2\n",
    "import time, glob, shutil, datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import csv, json\n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from syncnet_python.facetrack import *\n",
    "from syncnet_python.syncnet import *\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4baa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        super(VideoIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.avis = []\n",
    "        for utt in glob.glob(data_dir+'*'):\n",
    "            self.utts.append(utt)\n",
    "            frames_dir = utt+'/pyframes/'\n",
    "            if os.path.exists(frames_dir):\n",
    "                rmtree(frames_dir)\n",
    "            os.makedirs(frames_dir)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.load_frames(self.utts[i])\n",
    "    \n",
    "    def load_frames(self, utt):\n",
    "        \n",
    "        videofile = os.path.join(utt,'pyavi','video.avi')\n",
    "        output = os.path.join(utt,'pyframes','%06d.jpg')\n",
    "        command = f\"ffmpeg -loglevel quiet -y -i {videofile} -qscale:v 2 -threads 1 -f image2 {output}\"\n",
    "        output = subprocess.call(command, shell=True, stdout=None)\n",
    "        flist = glob.glob(utt+'/pyframes/*.jpg')\n",
    "        flist.sort()\n",
    "        frames = []\n",
    "        for fname in flist:\n",
    "            image = cv2.imread(fname)\n",
    "            frames.append(image)\n",
    "        return np.array(frames)\n",
    "\n",
    "def cut_into_utterances(filename, output_dir, maxWMER=1000):\n",
    "    \n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml\"\n",
    "    xmlfile = os.path.join(xmldir, filename+'.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    utterance_items = []\n",
    "    paths = glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/ptn*/**/{filename}*.ts\") \\\n",
    "    + glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/raw/{filename}*.ts\")\n",
    "    inputVideo = paths[0]\n",
    "    command_elems = [\"ffmpeg -loglevel quiet -y -i \" + inputVideo]\n",
    "    for item in root.findall('./body/segments/segment'):\n",
    "        if (item.attrib['id'].split('_')[-1]=='align' and float(item.attrib['WMER'])<=maxWMER):\n",
    "            if (float(item.attrib['endtime']) - float(item.attrib['starttime'])<2):\n",
    "                continue                        \n",
    "            location = output_dir + item.attrib['id']\n",
    "            ready_to_crop = prepare_output_directory(location)\n",
    "            if ready_to_crop:\n",
    "                utterance_items.append(item)\n",
    "                data = item.attrib\n",
    "                start = datetime.timedelta(seconds=float(data['starttime']))\n",
    "                end = datetime.timedelta(seconds=float(data['endtime']))\n",
    "                output = os.path.join(location, 'pyavi', 'video.avi')\n",
    "                command_elems.append(\" -ss \" + str(start) + \" -to \" + str(end) + \" -c copy \" + output) # -c:a mp3 -c:v mpeg4\n",
    "                create_transcript_from_XML(location, item)\n",
    "    command = \"\".join(command_elems)\n",
    "    s = time.time()\n",
    "    result = subprocess.run(command, shell=True, stdout=None)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"ERROR: ffmpeg failed to trim video: {filename}\")\n",
    "        print(f\"result: {result}\")\n",
    "    t = time.time() - s\n",
    "    print(f\"Took {t} seconds to trim {len(command_elems)-1} utterances\")\n",
    "    return utterance_items\n",
    "\n",
    "def getGenre(filename):\n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml/\"\n",
    "    xmlfile = os.path.join(xmldir, filename+'.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    head = root.find('./head/recording')\n",
    "    genre = head.attrib[\"genre\"]\n",
    "    return genre\n",
    "    \n",
    "def prepare_output_directory(location):\n",
    "    ready = True\n",
    "    incomplete_directory_exists = os.path.isdir(location) and not os.path.exists(f\"{location}/utterance_info.csv\")\n",
    "    if(incomplete_directory_exists):\n",
    "        shutil.rmtree(location)\n",
    "    elif(os.path.isdir(location)):\n",
    "        ready = False\n",
    "        return ready  #  This utterance has been processed already.\n",
    "    else:\n",
    "        pass\n",
    "    subprocess.run(\"mkdir -p \" + location + \"/pyavi/\", stdout=subprocess.DEVNULL, shell=True)    \n",
    "    return ready\n",
    "\n",
    "def create_transcript_from_XML(location, item):\n",
    "    # TODO: the transcript should only contain the words spoken in the final cropped video. \n",
    "    utterance = \"\"\n",
    "    for child in item:\n",
    "        utterance+=child.text + \" \"\n",
    "    data = item.attrib\n",
    "    data.update({\"utterance\": utterance})\n",
    "    with open(location + '/transcript.txt', 'w') as outfile:\n",
    "        outfile.write(str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94b8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/disk/scratch/s1768177/pipeline/output_data/'\n",
    "filelist = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/dev.full\"\n",
    "desired_genres = [\"drama\", \"childrens\", \"news\", \"documentary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9751dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20080508_173000_bbctwo_great_british_menu', '20080509_013000_bbcone_bill_oddie_s_wild_side']\n",
      "\n",
      "2021-07-29 00:10:57.132481. Cutting utterances from raw videos.\n",
      "1. 20080509_013000_bbcone_bill_oddie_s_wild_side. (documentary) \n",
      "Took 95.59221959114075 seconds to trim 212 utterances\n",
      "\n",
      "Finished Cutting total 212 utterances from 1 videos\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "with open(filelist, \"r\") as f:\n",
    "    files = f.read().split()\n",
    "files = files[15:17]\n",
    "print(files)\n",
    "print(f\"\\n{datetime.datetime.now()}. Cutting utterances from raw videos.\")\n",
    "total_utterances_processed = 0\n",
    "for filename in files:\n",
    "    genre = getGenre(filename)\n",
    "    if (genre in desired_genres):\n",
    "        print(f\"{count}. {filename}. ({genre}) \")\n",
    "        count += 1\n",
    "        utterance_items = cut_into_utterances(filename, data_dir)\n",
    "        total_utterances_processed += len(utterance_items)\n",
    "print(f\"\\nFinished Cutting total {total_utterances_processed} utterances from {count-1} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataset = VideoIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=24)\n",
    "facetrack = FaceTrack('cuda:3')\n",
    "for i, (utt, frames) in enumerate(dataloader):\n",
    "    print(i, utt.split('/')[-1], len(frames))\n",
    "    facetrack.run(data_dir=utt, frames=frames)\n",
    "    no_faces_found = len(os.listdir(utt + \"/pycrop/\")) == 0\n",
    "    if(no_faces_found):\n",
    "        shutil.rmtree(utt)\n",
    "        \n",
    "print(f\"Time taken: {(time.time()-start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SyncNetIterableDataset(torch.utils.data.IterableDataset):    \n",
    "#     def __init__(self, path):\n",
    "#         super(SyncNetIterableDataset).__init__()\n",
    "#         self.avis = []\n",
    "#         for avi in glob.glob(data_dir+'*/pycrop/*.avi'):\n",
    "#             self.avis.append(avi)\n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         worker_info = torch.utils.data.get_worker_info()\n",
    "#         if worker_info is None:\n",
    "#             offset = 0\n",
    "#             shift = 1\n",
    "#         else:\n",
    "#             offset = worker_info.id\n",
    "#             shift = worker_info.num_workers\n",
    "#         for i in range(offset, len(self.avis), shift):\n",
    "#             utt = self.avis[i].split('/pycrop/')[0]\n",
    "#             yield utt, self.avis[i], self.load_frames(self.avis[i])\n",
    "    \n",
    "#     def load_frames(self, videofile):\n",
    "#         cap = cv2.VideoCapture(videofile)\n",
    "#         frame_num = 1;\n",
    "#         frames = []\n",
    "#         while frame_num:\n",
    "#             frame_num += 1\n",
    "#             ret, image = cap.read()\n",
    "#             if ret == 0:\n",
    "#                 break\n",
    "#             frames.append(cv2.resize(image, (224, 224)))\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "#         frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "#         frames = np.stack(frames, axis=3)\n",
    "#         frames = np.transpose(frames, (2,3,0,1))\n",
    "#         frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "#         return frames\n",
    "    \n",
    "class SyncNetIterableDataset(torch.utils.data.IterableDataset):    \n",
    "   \n",
    "    def __init__(self, path):\n",
    "        super(SyncNetIterableDataset).__init__()\n",
    "        self.avis = []\n",
    "        for avi in glob.glob(path+'*/pycrop/*.avi'):\n",
    "            self.avis.append(avi)\n",
    "            tmp_dir = avi.split('pycrop')[0]+'/pytmp/'\n",
    "            if os.path.exists(tmp_dir):\n",
    "                rmtree(tmp_dir)\n",
    "            os.makedirs(tmp_dir)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.avis), shift):\n",
    "            utt = self.avis[i].split('/pycrop/')[0]\n",
    "            yield utt, self.avis[i], self.load_frames(self.avis[i]), self.load_audio(utt, self.avis[i])\n",
    "    \n",
    "    def load_audio(self, utt, videofile):\n",
    "        command = f\"ffmpeg -loglevel quiet -y -i {videofile} -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 {os.path.join(utt,'pytmp/audio.wav')}\"\n",
    "        output = subprocess.call(command, shell=True, stdout=None)\n",
    "        sample_rate, audio = wavfile.read(os.path.join(utt,'pytmp/audio.wav'))\n",
    "        return (sample_rate, audio)\n",
    "    \n",
    "    def load_frames(self, videofile):\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "        frame_num = 1;\n",
    "        frames = []\n",
    "        while frame_num:\n",
    "            frame_num += 1\n",
    "            ret, image = cap.read()\n",
    "            if ret == 0:\n",
    "                break\n",
    "            frames.append(cv2.resize(image, (224, 224)))\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "        frames = np.stack(frames, axis=3)\n",
    "        frames = np.transpose(frames, (2,3,0,1))\n",
    "        frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "        return frames\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = SyncNetIterableDataset(data_dir)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=16)\n",
    "# syncnet = SyncNet()\n",
    "# for i, (utt, avi, frames) in enumerate(dataloader):\n",
    "#     print(i, utt.split('/')[-1], avi.split('/')[-1], len(frames))\n",
    "#     syncnet.setup(utt)\n",
    "#     offset, conf, dist = syncnet.evaluate(avi,frames)\n",
    "#     print(offset, conf)\n",
    "\n",
    "dataset = SyncNetIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=24)\n",
    "syncnet = SyncNet()\n",
    "for i, (utt, avi, frames, (sample_rate, audio)) in enumerate(dataloader):\n",
    "    print(i, utt, avi.split('/')[-1], len(frames))\n",
    "    syncnet.setup(utt)\n",
    "    offset, conf, dist = syncnet.evaluate(avi,frames,sample_rate,audio)\n",
    "    print(offset, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(dataset_dir):\n",
    "    for utterance in os.listdir(dataset_dir):\n",
    "        source = os.path.join(dataset_dir, utterance, 'pycrop')\n",
    "        dest = os.path.join(dataset_dir, utterance)\n",
    "        for f in os.listdir(source):\n",
    "            new_path = shutil.move(f\"{source}/{f}\", f\"{dest}/{f}\")\n",
    "        for f in glob.glob(f\"{dest}/py*\"):\n",
    "            shutil.rmtree(f)\n",
    "            \n",
    "cleanup(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3887e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd583dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "videofile = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/video.avi\"\n",
    "# videofile_avi = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/tracks/video.avi\"\n",
    "# videofile = \"playground/lrs3test/00006.mp4\"\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "frames = []\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    if ret == 0:\n",
    "        break\n",
    "    frames.append(image)\n",
    "# print(f\"The shape of frames is: {np.array(frames).shape}\")\n",
    "# for i in range(len(frames)):\n",
    "#     cv2.imwrite(f\"playground/cv2images/{i}.jpg\", frames[i])\n",
    "print(len(frames))\n",
    "\n",
    "import subprocess\n",
    "# command = (\"ffmpeg -loglevel quiet -y -i %s -qscale:v 2 -async 1 -r 25 %s\" % (videofile, videofile_avi))\n",
    "# output = subprocess.call(command, shell=True, stdout=None)\n",
    "command = (\"ffmpeg -y -i %s -threads 1 -f image2 %s\" % (videofile,'playground/images/%06d.jpg')) \n",
    "output = subprocess.call(command, shell=True)\n",
    "flist = glob.glob('playground/images/*.jpg')\n",
    "flist.sort()\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59063e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flist = glob.glob('playground/images/*.jpg')\n",
    "# flist.sort()\n",
    "# for image in flist:\n",
    "#     print(cv2.imread(image).shape)\n",
    "#     print(frames[50].shape)\n",
    "#     if (cv2.imread(image) == frames[50]).all():\n",
    "#         print(\"got it!\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052999be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455b695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a15ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

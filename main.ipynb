{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c718eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import cv2\n",
    "import time, glob, shutil, datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import csv, json\n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from syncnet_python.facetrack import *\n",
    "from syncnet_python.syncnet import *\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4baa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        super(VideoIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.avis = []\n",
    "        for utt in glob.glob(data_dir+'*'):\n",
    "            self.utts.append(utt)\n",
    "            frames_dir = utt+'/pyframes/'\n",
    "            if os.path.exists(frames_dir):\n",
    "                rmtree(frames_dir)\n",
    "            os.makedirs(frames_dir)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.load_frames(self.utts[i])\n",
    "    \n",
    "    def load_frames(self, utt):\n",
    "        \n",
    "        videofile = os.path.join(utt,'pyavi','video.avi')\n",
    "        output = os.path.join(utt,'pyframes','%06d.jpg')\n",
    "        command = f\"ffmpeg -loglevel quiet -y -i {videofile} -qscale:v 2 -threads 1 -f image2 {output}\"\n",
    "        output = subprocess.call(command, shell=True, stdout=None)\n",
    "        flist = glob.glob(utt+'/pyframes/*.jpg')\n",
    "        flist.sort()\n",
    "        frames = []\n",
    "        for fname in flist:\n",
    "            image = cv2.imread(fname)\n",
    "            frames.append(image)\n",
    "        return np.array(frames)\n",
    "\n",
    "def cut_into_utterances(filename, output_dir, maxWMER=1000):\n",
    "    \n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml\"\n",
    "    xmlfile = os.path.join(xmldir, filename+'.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    utterance_items = []\n",
    "    paths = glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/ptn*/**/{filename}*.ts\") \\\n",
    "    + glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/raw/{filename}*.ts\")\n",
    "    inputVideo = paths[0]\n",
    "    command_elems = [\"ffmpeg -loglevel quiet -y -i \" + inputVideo]\n",
    "    for item in root.findall('./body/segments/segment'):\n",
    "        if (item.attrib['id'].split('_')[-1]=='align' and float(item.attrib['WMER'])<=maxWMER):\n",
    "            if (float(item.attrib['endtime']) - float(item.attrib['starttime'])<2):\n",
    "                continue                        \n",
    "            location = output_dir + item.attrib['id']\n",
    "            ready_to_crop = prepare_output_directory(location)\n",
    "            if ready_to_crop:\n",
    "                utterance_items.append(item)\n",
    "                data = item.attrib\n",
    "                start = datetime.timedelta(seconds=float(data['starttime']))\n",
    "                end = datetime.timedelta(seconds=float(data['endtime']))\n",
    "                output = os.path.join(location, 'pyavi', 'video.avi')\n",
    "                command_elems.append(\" -ss \" + str(start) + \" -to \" + str(end) + \" -c copy \" + output) # -c:a mp3 -c:v mpeg4\n",
    "                create_transcript_from_XML(location, item)\n",
    "    command = \"\".join(command_elems)\n",
    "    s = time.time()\n",
    "    result = subprocess.run(command, shell=True, stdout=None)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"ERROR: ffmpeg failed to trim video: {filename}\")\n",
    "        print(f\"result: {result}\")\n",
    "    t = time.time() - s\n",
    "    print(f\"Took {t} seconds to trim {len(command_elems)-1} utterances\")\n",
    "    return utterance_items\n",
    "\n",
    "def getGenre(filename):\n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml/\"\n",
    "    xmlfile = os.path.join(xmldir, filename+'.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    head = root.find('./head/recording')\n",
    "    genre = head.attrib[\"genre\"]\n",
    "    return genre\n",
    "    \n",
    "def prepare_output_directory(location):\n",
    "    ready = True\n",
    "    incomplete_directory_exists = os.path.isdir(location) and not os.path.exists(f\"{location}/utterance_info.csv\")\n",
    "    if(incomplete_directory_exists):\n",
    "        shutil.rmtree(location)\n",
    "    elif(os.path.isdir(location)):\n",
    "        ready = False\n",
    "        return ready  #  This utterance has been processed already.\n",
    "    else:\n",
    "        pass\n",
    "    subprocess.run(\"mkdir -p \" + location + \"/pyavi/\", stdout=subprocess.DEVNULL, shell=True)    \n",
    "    return ready\n",
    "\n",
    "def create_transcript_from_XML(location, item):\n",
    "    # TODO: the transcript should only contain the words spoken in the final cropped video. \n",
    "    utterance = \"\"\n",
    "    for child in item:\n",
    "        utterance+=child.text + \" \"\n",
    "    data = item.attrib\n",
    "    data.update({\"utterance\": utterance})\n",
    "    with open(location + '/transcript.txt', 'w') as outfile:\n",
    "        outfile.write(str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94b8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/disk/scratch/s1768177/pipeline/output_data/'\n",
    "filelist = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/train.short\"\n",
    "desired_genres = [\"drama\", \"childrens\", \"news\", \"documentary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9751dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-07-28 18:48:57.976369. Cutting utterances from raw videos.\n",
      "1. 20080505_000500_bbcone_weatherview. (news) \n",
      "Took 0.7967555522918701 seconds to trim 17 utterances\n",
      "\n",
      "Finished Cutting total 17 utterances from 1 videos\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "with open(filelist, \"r\") as f:\n",
    "    files = f.read().split()\n",
    "files = files[:1]\n",
    "print(f\"\\n{datetime.datetime.now()}. Cutting utterances from raw videos.\")\n",
    "total_utterances_processed = 0\n",
    "for filename in files:\n",
    "    genre = getGenre(filename)\n",
    "    if (genre in desired_genres):\n",
    "        print(f\"{count}. {filename}. ({genre}) \")\n",
    "        count += 1\n",
    "        utterance_items = cut_into_utterances(filename, data_dir)\n",
    "        total_utterances_processed += len(utterance_items)\n",
    "print(f\"\\nFinished Cutting total {total_utterances_processed} utterances from {count-1} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae0e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S3FD] loading with cuda:3\n",
      "[S3FD] finished loading (0.3044 sec)\n",
      "0 ID20080505_000500_bbcone_weatherview_utt_9_align 233\n",
      "1 ID20080505_000500_bbcone_weatherview_utt_8_align 655\n",
      "2 ID20080505_000500_bbcone_weatherview_utt_12_align 98\n",
      "3 ID20080505_000500_bbcone_weatherview_utt_4_align 286\n",
      "4 ID20080505_000500_bbcone_weatherview_utt_6_align 122\n",
      "5 ID20080505_000500_bbcone_weatherview_utt_15_align 526\n",
      "6 ID20080505_000500_bbcone_weatherview_utt_13_align 261\n",
      "7 ID20080505_000500_bbcone_weatherview_utt_20_align 93\n",
      "8 ID20080505_000500_bbcone_weatherview_utt_2_align 137\n",
      "9 ID20080505_000500_bbcone_weatherview_utt_7_align 197\n",
      "10 ID20080505_000500_bbcone_weatherview_utt_19_align 142\n",
      "11 ID20080505_000500_bbcone_weatherview_utt_11_align 90\n",
      "12 ID20080505_000500_bbcone_weatherview_utt_21_align 65\n",
      "13 ID20080505_000500_bbcone_weatherview_utt_10_align 354\n",
      "14 ID20080505_000500_bbcone_weatherview_utt_3_align 279\n",
      "15 ID20080505_000500_bbcone_weatherview_utt_14_align 259\n",
      "16 ID20080505_000500_bbcone_weatherview_utt_5_align 203\n",
      "Time taken: 1.60 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = VideoIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=24)\n",
    "facetrack = FaceTrack('cuda:3')\n",
    "for i, (utt, frames) in enumerate(dataloader):\n",
    "    print(i, utt.split('/')[-1], len(frames))\n",
    "    facetrack.run(data_dir=utt, frames=frames)\n",
    "    no_faces_found = len(os.listdir(utt + \"/pycrop/\")) == 0\n",
    "    if(no_faces_found):\n",
    "        shutil.rmtree(utt)\n",
    "        \n",
    "print(f\"Time taken: {(time.time()-start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a0e8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SyncNetIterableDataset(torch.utils.data.IterableDataset):    \n",
    "#     def __init__(self, path):\n",
    "#         super(SyncNetIterableDataset).__init__()\n",
    "#         self.avis = []\n",
    "#         for avi in glob.glob(data_dir+'*/pycrop/*.avi'):\n",
    "#             self.avis.append(avi)\n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         worker_info = torch.utils.data.get_worker_info()\n",
    "#         if worker_info is None:\n",
    "#             offset = 0\n",
    "#             shift = 1\n",
    "#         else:\n",
    "#             offset = worker_info.id\n",
    "#             shift = worker_info.num_workers\n",
    "#         for i in range(offset, len(self.avis), shift):\n",
    "#             utt = self.avis[i].split('/pycrop/')[0]\n",
    "#             yield utt, self.avis[i], self.load_frames(self.avis[i])\n",
    "    \n",
    "#     def load_frames(self, videofile):\n",
    "#         cap = cv2.VideoCapture(videofile)\n",
    "#         frame_num = 1;\n",
    "#         frames = []\n",
    "#         while frame_num:\n",
    "#             frame_num += 1\n",
    "#             ret, image = cap.read()\n",
    "#             if ret == 0:\n",
    "#                 break\n",
    "#             frames.append(cv2.resize(image, (224, 224)))\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "#         frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "#         frames = np.stack(frames, axis=3)\n",
    "#         frames = np.transpose(frames, (2,3,0,1))\n",
    "#         frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "#         return frames\n",
    "    \n",
    "class SyncNetIterableDataset(torch.utils.data.IterableDataset):    \n",
    "   \n",
    "    def __init__(self, path):\n",
    "        super(SyncNetIterableDataset).__init__()\n",
    "        self.avis = []\n",
    "        for avi in glob.glob(path+'*/pycrop/*.avi'):\n",
    "            self.avis.append(avi)\n",
    "            tmp_dir = avi.split('pycrop')[0]+'/pytmp/'\n",
    "            if os.path.exists(tmp_dir):\n",
    "                rmtree(tmp_dir)\n",
    "            os.makedirs(tmp_dir)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.avis), shift):\n",
    "            utt = self.avis[i].split('/pycrop/')[0]\n",
    "            yield utt, self.avis[i], self.load_frames(self.avis[i]), self.load_audio(utt, self.avis[i])\n",
    "    \n",
    "    def load_audio(self, utt, videofile):\n",
    "        command = f\"ffmpeg -loglevel quiet -y -i {videofile} -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 {os.path.join(utt,'pytmp/audio.wav')}\"\n",
    "        output = subprocess.call(command, shell=True, stdout=None)\n",
    "        sample_rate, audio = wavfile.read(os.path.join(utt,'pytmp/audio.wav'))\n",
    "        return (sample_rate, audio)\n",
    "    \n",
    "    def load_frames(self, videofile):\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "        frame_num = 1;\n",
    "        frames = []\n",
    "        while frame_num:\n",
    "            frame_num += 1\n",
    "            ret, image = cap.read()\n",
    "            if ret == 0:\n",
    "                break\n",
    "            frames.append(cv2.resize(image, (224, 224)))\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "        frames = np.stack(frames, axis=3)\n",
    "        frames = np.transpose(frames, (2,3,0,1))\n",
    "        frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "        return frames\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5625a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model syncnet_python/data/syncnet_v2.model loaded.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/afs/inf.ed.ac.uk/user/s17/s1768177/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/afs/inf.ed.ac.uk/user/s17/s1768177/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 34, in fetch\n    data = next(self.dataset_iter)\n  File \"<ipython-input-23-6a5644c63dd3>\", line 56, in __iter__\n    yield utt, self.avis[i], self.load_frames(self.avis[i]), self.load_audio(utt, self.avis[i])\n  File \"<ipython-input-23-6a5644c63dd3>\", line 61, in load_audio\n    sample_rate, audio = wavfile.read(os.path.join(utt,'pytmp/audio.wav'))\n  File \"/afs/inf.ed.ac.uk/user/s17/s1768177/miniconda3/envs/avsr/lib/python3.7/site-packages/scipy/io/wavfile.py\", line 647, in read\n    fid = open(filename, 'rb')\nFileNotFoundError: [Errno 2] No such file or directory: '/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pytmp/audio.wav'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f75586f7ddb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msyncnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSyncNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msyncnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/afs/inf.ed.ac.uk/user/s17/s1768177/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/afs/inf.ed.ac.uk/user/s17/s1768177/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 34, in fetch\n    data = next(self.dataset_iter)\n  File \"<ipython-input-23-6a5644c63dd3>\", line 56, in __iter__\n    yield utt, self.avis[i], self.load_frames(self.avis[i]), self.load_audio(utt, self.avis[i])\n  File \"<ipython-input-23-6a5644c63dd3>\", line 61, in load_audio\n    sample_rate, audio = wavfile.read(os.path.join(utt,'pytmp/audio.wav'))\n  File \"/afs/inf.ed.ac.uk/user/s17/s1768177/miniconda3/envs/avsr/lib/python3.7/site-packages/scipy/io/wavfile.py\", line 647, in read\n    fid = open(filename, 'rb')\nFileNotFoundError: [Errno 2] No such file or directory: '/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pytmp/audio.wav'\n"
     ]
    }
   ],
   "source": [
    "# dataset = SyncNetIterableDataset(data_dir)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=16)\n",
    "# syncnet = SyncNet()\n",
    "# for i, (utt, avi, frames) in enumerate(dataloader):\n",
    "#     print(i, utt.split('/')[-1], avi.split('/')[-1], len(frames))\n",
    "#     syncnet.setup(utt)\n",
    "#     offset, conf, dist = syncnet.evaluate(avi,frames)\n",
    "#     print(offset, conf)\n",
    "\n",
    "dataset = SyncNetIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=24)\n",
    "syncnet = SyncNet()\n",
    "for i, (utt, avi, frames, (sample_rate, audio)) in enumerate(dataloader):\n",
    "    print(i, utt, avi.split('/')[-1], len(frames))\n",
    "    syncnet.setup(utt)\n",
    "    offset, conf, dist = syncnet.evaluate(avi,frames,sample_rate,audio)\n",
    "    print(offset, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dd0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(dataset_dir):\n",
    "    for utterance in os.listdir(dataset_dir):\n",
    "        source = os.path.join(dataset_dir, utterance, 'pycrop')\n",
    "        dest = os.path.join(dataset_dir, utterance)\n",
    "        for f in os.listdir(source):\n",
    "            new_path = shutil.move(f\"{source}/{f}\", f\"{dest}/{f}\")\n",
    "        for f in glob.glob(f\"{dest}/py*\"):\n",
    "            shutil.rmtree(f)\n",
    "            \n",
    "cleanup(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3887e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd583dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "videofile = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/video.avi\"\n",
    "# videofile_avi = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/tracks/video.avi\"\n",
    "# videofile = \"playground/lrs3test/00006.mp4\"\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "frames = []\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    if ret == 0:\n",
    "        break\n",
    "    frames.append(image)\n",
    "# print(f\"The shape of frames is: {np.array(frames).shape}\")\n",
    "# for i in range(len(frames)):\n",
    "#     cv2.imwrite(f\"playground/cv2images/{i}.jpg\", frames[i])\n",
    "print(len(frames))\n",
    "\n",
    "import subprocess\n",
    "# command = (\"ffmpeg -loglevel quiet -y -i %s -qscale:v 2 -async 1 -r 25 %s\" % (videofile, videofile_avi))\n",
    "# output = subprocess.call(command, shell=True, stdout=None)\n",
    "command = (\"ffmpeg -y -i %s -threads 1 -f image2 %s\" % (videofile,'playground/images/%06d.jpg')) \n",
    "output = subprocess.call(command, shell=True)\n",
    "flist = glob.glob('playground/images/*.jpg')\n",
    "flist.sort()\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59063e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flist = glob.glob('playground/images/*.jpg')\n",
    "# flist.sort()\n",
    "# for image in flist:\n",
    "#     print(cv2.imread(image).shape)\n",
    "#     print(frames[50].shape)\n",
    "#     if (cv2.imread(image) == frames[50]).all():\n",
    "#         print(\"got it!\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052999be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455b695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a15ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

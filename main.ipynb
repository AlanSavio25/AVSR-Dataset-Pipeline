{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c718eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import cv2\n",
    "import time, glob, shutil, datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import csv, json\n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from facetrack import *\n",
    "from syncnet import *\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4baa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        super(VideoIterableDataset).__init__()\n",
    "        self.utts = []\n",
    "        self.avis = []\n",
    "        for utt in glob.glob(data_dir+'*'):\n",
    "            self.utts.append(utt)\n",
    "            frames_dir = utt+'/pyframes/'\n",
    "            if os.path.exists(frames_dir):\n",
    "                rmtree(frames_dir)\n",
    "            os.makedirs(frames_dir)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.utts), shift):\n",
    "            yield self.utts[i], self.load_frames(self.utts[i])\n",
    "    \n",
    "    def load_frames(self, utt):\n",
    "        \n",
    "        videofile = os.path.join(utt,'pyavi','video.avi')\n",
    "        output = os.path.join(utt,'pyframes','%06d.jpg')\n",
    "        command = f\"ffmpeg -loglevel quiet -y -i {videofile} -qscale:v 2 -threads 1 -f image2 {output}\"\n",
    "        output = subprocess.call(command, shell=True, stdout=None)\n",
    "        flist = glob.glob(utt+'/pyframes/*.jpg')\n",
    "        flist.sort()\n",
    "        frames = []\n",
    "        for fname in flist:\n",
    "            image = cv2.imread(fname)\n",
    "            frames.append(image)\n",
    "        return np.array(frames)\n",
    "    \n",
    "\n",
    "def cut_into_utterances(filename, output_dir, genre, maxWMER=1000):\n",
    "    \n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml\"\n",
    "    xmlfile = os.path.join(xmldir, filename+'.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    utterance_items = []\n",
    "    paths = glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/ptn*/**/{filename}*.ts\") \\\n",
    "    + glob.glob(f\"/afs/inf.ed.ac.uk/group/project/nst/bbcdata/raw/{filename}*.ts\")\n",
    "    inputVideo = paths[0]\n",
    "    command_elems = [\"ffmpeg -loglevel quiet -y -i \" + inputVideo]\n",
    "    for item in root.findall('./body/segments/segment'):\n",
    "        if (item.attrib['id'].split('_')[-1]=='align' and float(item.attrib['WMER'])<=maxWMER):\n",
    "            if (float(item.attrib['endtime']) - float(item.attrib['starttime'])<2):\n",
    "                continue                        \n",
    "            location = output_dir + item.attrib['id']\n",
    "            ready_to_crop = prepare_output_directory(location)\n",
    "            if ready_to_crop:\n",
    "                utterance_items.append(item)\n",
    "                data = item.attrib\n",
    "                start = datetime.timedelta(seconds=float(data['starttime']))\n",
    "                end = datetime.timedelta(seconds=float(data['endtime']))\n",
    "                output = os.path.join(location, 'pyavi', 'video.avi')\n",
    "                command_elems.append(\" -ss \" + str(start) + \" -to \" + str(end) + \" -c copy \" + output) # -c:a mp3 -c:v mpeg4\n",
    "                create_transcript_from_XML(location, item, genre)\n",
    "    command = \"\".join(command_elems)\n",
    "    s = time.time()\n",
    "    result = subprocess.run(command, shell=True, stdout=None)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"ERROR: ffmpeg failed to trim video: {filename}\")\n",
    "        print(f\"result: {result}\")\n",
    "    t = time.time() - s\n",
    "    print(f\"Took {t} seconds to trim {len(command_elems)-1} utterances\")\n",
    "    return utterance_items\n",
    "\n",
    "def get_genre(filename):\n",
    "    xmldir = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/data/xml/\"\n",
    "    xmlfile = os.path.join(xmldir, filename+'.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    head = root.find('./head/recording')\n",
    "    genre = head.attrib[\"genre\"]\n",
    "    return genre\n",
    "    \n",
    "def prepare_output_directory(location):\n",
    "    ready = True\n",
    "    incomplete_directory_exists = os.path.isdir(location) and not os.path.exists(f\"{location}/0*.txt\")\n",
    "    if(incomplete_directory_exists):\n",
    "        shutil.rmtree(location)\n",
    "    elif(os.path.isdir(location)):\n",
    "        ready = False\n",
    "        return ready  #  This utterance has been processed already.\n",
    "    else:\n",
    "        pass\n",
    "    subprocess.run(\"mkdir -p \" + location + \"/pyavi/\", stdout=subprocess.DEVNULL, shell=True)    \n",
    "    return ready\n",
    "\n",
    "def create_transcript_from_XML(location, item, genre):\n",
    "    # TODO: the transcript should only contain the words spoken in the final cropped video. \n",
    "    utterance = \"\"\n",
    "    for child in item:\n",
    "        utterance+=child.text + \" \"\n",
    "    data = item.attrib\n",
    "    data.update({\"utterance\": utterance})\n",
    "    data.update({\"genre\": genre})\n",
    "    with open(location + '/transcript.txt', 'w') as outfile:\n",
    "        outfile.write(str(data))\n",
    "        \n",
    "def cleanup(dataset_dir):\n",
    "    for utterance in os.listdir(dataset_dir):\n",
    "        source = os.path.join(dataset_dir, utterance, 'pycrop')\n",
    "        dest = os.path.join(dataset_dir, utterance)\n",
    "        for f in os.listdir(source):\n",
    "            new_path = shutil.move(f\"{source}/{f}\", f\"{dest}/{f}\")\n",
    "        for f in glob.glob(f\"{dest}/py*\"):\n",
    "            shutil.rmtree(f)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94b8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/disk/scratch/s1768177/pipeline/test_output_data/'\n",
    "filelist = \"/afs/inf.ed.ac.uk/group/cstr/datawww/asru/MGB1/scripts/dev.full\"\n",
    "desired_genres = [\"drama\", \"childrens\", \"news\", \"documentary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crop utterances\n",
    "count = 1\n",
    "with open(filelist, \"r\") as f:\n",
    "    files = f.read().split()\n",
    "    files = files[:3]\n",
    "print(f\"Cutting utterances from raw videos.\")\n",
    "total_utterances_processed = 0\n",
    "for filename in files:\n",
    "    genre = get_genre(filename)\n",
    "    if (genre in desired_genres):\n",
    "        print(f\"{count}. {filename}. ({genre}) \")\n",
    "        count += 1\n",
    "        utterance_items = cut_into_utterances(filename, data_dir, genre)\n",
    "        total_utterances_processed += len(utterance_items)\n",
    "print(f\"\\nFinished cutting total {total_utterances_processed} utterances from {count-1} videos\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataset = VideoIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=24)\n",
    "facetrack = FaceTrack('cuda:3')\n",
    "for i, (utt, frames) in enumerate(dataloader):\n",
    "    print(i, utt.split('/')[-1], len(frames))\n",
    "    facetrack.run(data_dir=utt, frames=frames)\n",
    "    no_faces_found = len(os.listdir(utt + \"/pycrop/\")) == 0\n",
    "    if(no_faces_found):\n",
    "        shutil.rmtree(utt)\n",
    "        \n",
    "print(f\"Time taken: {(time.time()-start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0e8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyncNetIterableDataset(torch.utils.data.IterableDataset):    \n",
    "   \n",
    "    def __init__(self, path):\n",
    "        super(SyncNetIterableDataset).__init__()\n",
    "        self.avis = []\n",
    "        for avi in glob.glob(path+'*/*.avi'):\n",
    "            self.avis.append(avi)\n",
    "            tmp_dir = os.path.dirname(avi)+'/pytmp/'\n",
    "            if os.path.exists(tmp_dir):\n",
    "                rmtree(tmp_dir)\n",
    "            os.makedirs(tmp_dir)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "\n",
    "        if worker_info is None:\n",
    "            offset = 0\n",
    "            shift = 1\n",
    "        else:\n",
    "            offset = worker_info.id\n",
    "            shift = worker_info.num_workers\n",
    "        for i in range(offset, len(self.avis), shift):\n",
    "            utt = os.path.dirname(self.avis[i])\n",
    "            yield utt, self.avis[i], self.load_frames(self.avis[i]), self.load_audio(utt, self.avis[i])\n",
    "    \n",
    "    def load_audio(self, utt, videofile):\n",
    "        command = f\"ffmpeg -loglevel quiet -y -i {videofile} -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 {os.path.join(utt,'pytmp/audio.wav')}\"\n",
    "        output = subprocess.call(command, shell=True, stdout=None)\n",
    "        sample_rate, audio = wavfile.read(os.path.join(utt,'pytmp/audio.wav'))\n",
    "        return (sample_rate, audio)\n",
    "    \n",
    "    def load_frames(self, videofile):\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "        frame_num = 1;\n",
    "        frames = []\n",
    "        while frame_num:\n",
    "            frame_num += 1\n",
    "            ret, image = cap.read()\n",
    "            if ret == 0:\n",
    "                break\n",
    "            frames.append(cv2.resize(image, (224, 224)))\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        frames = [frames[0], frames[0]] + frames + [frames[-1], frames[-1]]\n",
    "        frames = np.stack(frames, axis=3)\n",
    "        frames = np.transpose(frames, (2,3,0,1))\n",
    "        frames = np.array([frames[:,i:i+5,:,:] for i in range(0, frames.shape[1] - 4)], dtype='float32')\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5625a37b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model syncnet_model/syncnet_v2.model loaded.\n",
      "0 ID20080505_193000_bbcone_panorama_utt_94_align 00000.avi 345\n",
      "0 5.4118824\n",
      "1 ID20080505_193000_bbcone_panorama_utt_47_align 00000.avi 157\n",
      "1 7.5473156\n",
      "2 ID20080505_193000_bbcone_panorama_utt_101_align 00000.avi 121\n",
      "0 4.1719084\n",
      "3 ID20080505_193000_bbcone_panorama_utt_121_align 00000.avi 388\n",
      "0 5.236251\n",
      "4 ID20080505_193000_bbcone_panorama_utt_15_align 00000.avi 291\n",
      "2 7.446749\n",
      "5 ID20080505_193000_bbcone_panorama_utt_12_align 00000.avi 250\n",
      "1 9.182074\n",
      "6 ID20080505_193000_bbcone_panorama_utt_74_align 00001.avi 177\n",
      "0 0.2041874\n",
      "7 ID20080505_193000_bbcone_panorama_utt_74_align 00000.avi 130\n",
      "-15 0.7164364\n",
      "8 ID20080505_193000_bbcone_panorama_utt_74_align 00002.avi 123\n",
      "-9 0.41899586\n",
      "9 ID20080505_193000_bbcone_panorama_utt_105_align 00000.avi 660\n",
      "0 5.5470963\n",
      "10 ID20080505_193000_bbcone_panorama_utt_14_align 00000.avi 133\n",
      "2 5.7118607\n",
      "11 ID20080505_193000_bbcone_panorama_utt_10_align 00001.avi 206\n",
      "15 0.19881153\n",
      "12 ID20080505_193000_bbcone_panorama_utt_10_align 00000.avi 206\n",
      "-15 0.34924412\n",
      "13 ID20080505_193000_bbcone_panorama_utt_10_align 00002.avi 206\n",
      "-15 0.25370693\n",
      "14 ID20080505_193000_bbcone_panorama_utt_10_align 00003.avi 206\n",
      "0 6.4121666\n",
      "15 ID20080505_193000_bbcone_panorama_utt_118_align 00000.avi 135\n",
      "1 5.5020847\n",
      "16 ID20080505_193000_bbcone_panorama_utt_119_align 00001.avi 169\n",
      "1 8.721685\n",
      "17 ID20080505_193000_bbcone_panorama_utt_119_align 00000.avi 138\n",
      "5 1.4991331\n",
      "18 ID20080505_193000_bbcone_panorama_utt_119_align 00002.avi 133\n",
      "12 0.17296791\n",
      "19 ID20080505_193000_bbcone_panorama_utt_50_align 00000.avi 107\n",
      "-14 0.35896397\n",
      "20 ID20080505_193000_bbcone_panorama_utt_46_align 00000.avi 165\n",
      "2 6.349268\n",
      "21 ID20080505_193000_bbcone_panorama_utt_125_align 00000.avi 464\n",
      "1 8.464134\n",
      "22 ID20080505_193000_bbcone_panorama_utt_45_align 00000.avi 138\n",
      "0 5.533062\n",
      "23 ID20080505_193000_bbcone_panorama_utt_29_align 00000.avi 236\n",
      "15 0.28373146\n",
      "24 ID20080505_193000_bbcone_panorama_utt_96_align 00000.avi 109\n",
      "-10 0.68980885\n",
      "25 ID20080505_193000_bbcone_panorama_utt_79_align 00000.avi 131\n",
      "13 0.427639\n",
      "26 ID20080505_193000_bbcone_panorama_utt_26_align 00000.avi 152\n",
      "2 7.2330537\n",
      "27 ID20080505_193000_bbcone_panorama_utt_62_align 00001.avi 202\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 7399) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-61fe702d9d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msyncnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyncnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken: {(time.time()-start)/60:.2f} minutes\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/inf.ed.ac.uk/group/ug4-projects/s1768177/AVSR/pipeline/syncnet.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, videofile, frames, sample_rate, audio)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mim_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mimtv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvframe\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mim_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mim_out\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_lip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mim_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/avsr/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 7399) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit."
     ]
    }
   ],
   "source": [
    "# dataset = SyncNetIterableDataset(data_dir)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=16)\n",
    "# syncnet = SyncNet()\n",
    "# for i, (utt, avi, frames) in enumerate(dataloader):\n",
    "#     print(i, utt.split('/')[-1], avi.split('/')[-1], len(frames))\n",
    "#     syncnet.setup(utt)\n",
    "#     offset, conf, dist = syncnet.evaluate(avi,frames)\n",
    "#     print(offset, conf)\n",
    "\n",
    "start = time.time()\n",
    "dataset = SyncNetIterableDataset(data_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False, num_workers=20)\n",
    "syncnet = SyncNet()\n",
    "for i, (utt, avi, frames, (sample_rate, audio)) in enumerate(dataloader):\n",
    "    print(i, utt.split('/')[-1], avi.split('/')[-1], len(frames))\n",
    "    syncnet.setup(utt)\n",
    "    offset, conf, dist = syncnet.evaluate(avi,frames,sample_rate,audio)\n",
    "    print(offset, conf)\n",
    "print(f\"Time taken: {(time.time()-start)/60:.2f} minutes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(dataset_dir):\n",
    "    for utterance in os.listdir(dataset_dir):\n",
    "        source = os.path.join(dataset_dir, utterance, 'pycrop')\n",
    "        dest = os.path.join(dataset_dir, utterance)\n",
    "        for f in os.listdir(source):\n",
    "            new_path = shutil.move(f\"{source}/{f}\", f\"{dest}/{f}\")\n",
    "        for f in glob.glob(f\"{dest}/py*\"):\n",
    "            shutil.rmtree(f)\n",
    "            \n",
    "cleanup(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3887e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd583dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "videofile = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/video.avi\"\n",
    "# videofile_avi = \"/disk/scratch/s1768177/pipeline/output_data/ID20080505_000500_bbcone_weatherview_utt_8_align/pyavi/tracks/video.avi\"\n",
    "# videofile = \"playground/lrs3test/00006.mp4\"\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "frames = []\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    if ret == 0:\n",
    "        break\n",
    "    frames.append(image)\n",
    "# print(f\"The shape of frames is: {np.array(frames).shape}\")\n",
    "# for i in range(len(frames)):\n",
    "#     cv2.imwrite(f\"playground/cv2images/{i}.jpg\", frames[i])\n",
    "print(len(frames))\n",
    "\n",
    "import subprocess\n",
    "# command = (\"ffmpeg -loglevel quiet -y -i %s -qscale:v 2 -async 1 -r 25 %s\" % (videofile, videofile_avi))\n",
    "# output = subprocess.call(command, shell=True, stdout=None)\n",
    "command = (\"ffmpeg -y -i %s -threads 1 -f image2 %s\" % (videofile,'playground/images/%06d.jpg')) \n",
    "output = subprocess.call(command, shell=True)\n",
    "flist = glob.glob('playground/images/*.jpg')\n",
    "flist.sort()\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59063e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flist = glob.glob('playground/images/*.jpg')\n",
    "# flist.sort()\n",
    "# for image in flist:\n",
    "#     print(cv2.imread(image).shape)\n",
    "#     print(frames[50].shape)\n",
    "#     if (cv2.imread(image) == frames[50]).all():\n",
    "#         print(\"got it!\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052999be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455b695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd85702",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all transcripts have genre in them\n",
    "\n",
    "for f in glob.glob(data_dir+\"*/transcript.txt\"):\n",
    "    with open(f,'r') as t:\n",
    "        lines = t.read()\n",
    "        if 'genre' not in lines:\n",
    "            print(\"Error!\")\n",
    "print(lines)\n",
    "print('genre' in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7e4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
